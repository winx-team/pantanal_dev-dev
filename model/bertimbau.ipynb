{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Código responsável pela criação e desenvolvimento do modelo\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_neural = 'neuralmind/bert-base-portuguese-cased'\n",
    "diretorio_trabalho = 'C:/Users/Pedro/Documents/pantanal_dev/pantanal_dev-dev/'\n",
    "arquivo_csv = 'trusted_zone/datasets/infomoney_pronto.xlsx'\n",
    "coluna_indice = 'indice'\n",
    "colunas_remover = [\n",
    "    'data_publicacao', \n",
    "    'assunto', \n",
    "    'link_noticia', \n",
    "    'data_noticia', \n",
    "    'label'\n",
    "    ]\n",
    "numero_tuplas_dataset = 10000\n",
    "tamanho_bytes = 128\n",
    "tamanho_batch = 64\n",
    "tensor_bits = tf.int32\n",
    "camadas = 3\n",
    "porcentagem_treinamento = 0.80\n",
    "taxa_aprendizagem = 2e-5\n",
    "epocas = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier', 'bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizador = BertTokenizer.from_pretrained(modelo_neural)\n",
    "modelo = TFBertForSequenceClassification.from_pretrained(modelo_neural, num_labels=camadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegar_dataset():\n",
    "    return pd.read_excel(diretorio_trabalho + arquivo_csv)\n",
    "\n",
    "def formatar_dataset(dataset):\n",
    "    return dataset.drop(colunas_remover, axis=1)\n",
    "\n",
    "def cortar_dataset(dataset):\n",
    "    return dataset.head(numero_tuplas_dataset)\n",
    "\n",
    "def detalhes_dataset(total, negativos, neutros, positivos, corte):\n",
    "    print(\"\\nTotal Inserido: {}\".format(total))\n",
    "    print(\"\\nNegativos: {}\".format(negativos))\n",
    "    print(\"Neutros: {}\".format(neutros))\n",
    "    print(\"Positivos: {}\".format(positivos))\n",
    "    print(\"\\nCorte: {}\".format(corte))\n",
    "    print(\"Total Treinavel: {}\".format(corte * 3))\n",
    "\n",
    "def balancear_dataset(dataset):\n",
    "    tuplas_negativas = dataset.loc[dataset['classificacao'] == 'negativo']\n",
    "    tuplas_neutras = dataset.loc[dataset['classificacao'] == 'neutro']\n",
    "    tuplas_positivas = dataset.loc[dataset['classificacao'] == 'positivo']\n",
    "\n",
    "    numero_negativos = len(tuplas_negativas)\n",
    "    numero_neutros = len(tuplas_neutras)\n",
    "    numero_positivos = len(tuplas_positivas)\n",
    "\n",
    "    corte_menor = min(numero_negativos, numero_neutros, numero_positivos)\n",
    "\n",
    "    detalhes_dataset(len(dataset), numero_negativos, numero_neutros, numero_positivos, corte_menor)\n",
    "\n",
    "    corte_negativos = tuplas_negativas.head(corte_menor)\n",
    "    corte_neutros = tuplas_neutras.head(corte_menor)\n",
    "    corte_positivos = tuplas_positivas.head(corte_menor)\n",
    "\n",
    "    dataset_concatenado = pd.concat([corte_negativos, corte_neutros, corte_positivos])\n",
    "    dataset_sorteado = dataset_concatenado.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return dataset_sorteado\n",
    "\n",
    "def tamanho_sentencas(dataset):\n",
    "    quantidade_caracteres = []\n",
    "\n",
    "    for indice, tupla in dataset.iterrows():\n",
    "      quantidade_caracteres.append(len(tupla['titulo']))\n",
    "\n",
    "    caracter_minimo = min(quantidade_caracteres)\n",
    "    caracter_maximo = max(quantidade_caracteres)\n",
    "    caracter_medio = int(sum(quantidade_caracteres) / len(quantidade_caracteres))\n",
    "\n",
    "    print('\\nCaracteres Mínimo: {}'.format(caracter_minimo))\n",
    "    print('Caracteres Médio: {}'.format(caracter_medio))\n",
    "    print('Caracteres Máximo: {}'.format(caracter_maximo))\n",
    "\n",
    "def converter_labels(labels):\n",
    "    labels_saida = []\n",
    "\n",
    "    for label in labels:\n",
    "        if (label == 'negativo'):\n",
    "            labels_saida.append(0)\n",
    "        elif (label == 'neutro'):\n",
    "            labels_saida.append(1)\n",
    "        elif (label == 'positivo'):\n",
    "            labels_saida.append(2)\n",
    "        else:\n",
    "            labels_saida.append(-1)\n",
    "            print('ERRO')\n",
    "    \n",
    "  return labels_saida\n",
    "\n",
    "def gerador():\n",
    "    for exemplo in dados_tokenizados:\n",
    "        yield (\n",
    "            {\n",
    "                'input_ids': exemplo['input_ids'],\n",
    "                'attention_mask': exemplo['attention_mask']\n",
    "            },\n",
    "            exemplo['labels']\n",
    "            )\n",
    "\n",
    "def interpertar_predicao(predicao):\n",
    "    if (predicao == 0):\n",
    "        return 'Negativo'\n",
    "    elif (predicao == 1):\n",
    "        return 'Neutro'\n",
    "    elif (predicao == 2):\n",
    "        return 'Positivo'\n",
    "    else:\n",
    "        return 'ERRO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Inserido: 5049\n",
      "\n",
      "Negativos: 1080\n",
      "Neutros: 3009\n",
      "Positivos: 960\n",
      "\n",
      "Corte: 960\n",
      "Total Treinavel: 2880\n",
      "\n",
      "Caracteres Mínimo: 5\n",
      "Caracteres Médio: 73\n",
      "Caracteres Máximo: 132\n"
     ]
    }
   ],
   "source": [
    "dataset = pegar_dataset()\n",
    "dataset = formatar_dataset(dataset)\n",
    "dataset = balancear_dataset(dataset)\n",
    "\n",
    "tamanho_sentencas(dataset)\n",
    "\n",
    "sentencas = dataset['titulo'].to_list()\n",
    "labels = dataset['classificacao'].to_list()\n",
    "labels = converter_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentença: Reclamações contra bancos aumentam 17% em fevereiro, aponta BC\n",
      "   Tokens: ['Rec', '##lama', '##ções', 'contra', 'bancos', 'aumentam', '17', '%', 'em', 'fevereiro', ',', 'aponta', 'B', '##C']\n",
      ".     IDs: [2325, 4839, 315, 598, 9213, 20958, 1040, 110, 173, 1812, 117, 12110, 241, 22304]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizador.tokenize(sentencas[2])\n",
    "token_ids = tokenizador.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentença: {sentencas[2]}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'.     IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "mascaras = []\n",
    "dados_tokenizados = []\n",
    "\n",
    "for sentenca_unitaria, label_unitario in zip(sentencas, labels):\n",
    "    sentenca_tokenizada = tokenizador.encode_plus(\n",
    "        sentenca_unitaria,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        max_length=tamanho_bytes,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "        )\n",
    "    \n",
    "    entrada = {\n",
    "        'input_ids': sentenca_tokenizada['input_ids'][0],\n",
    "        'attention_mask': sentenca_tokenizada['attention_mask'][0],\n",
    "        'labels': label_unitario\n",
    "        }\n",
    "    \n",
    "    dados_tokenizados.append(entrada)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    gerador,\n",
    "    (\n",
    "        {'input_ids': tensor_bits, 'attention_mask': tensor_bits},\n",
    "        tensor_bits\n",
    "    ),\n",
    "    (\n",
    "        {\n",
    "            'input_ids': tf.TensorShape([tamanho_bytes]),\n",
    "            'attention_mask': tf.TensorShape([tamanho_bytes])\n",
    "        },\n",
    "        tf.TensorShape([])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho_treinamento = int(len(dados_tokenizados) * porcentagem_treinamento)\n",
    "dataset_treinamento = dataset.take(tamanho_treinamento).shuffle(tamanho_treinamento).batch(tamanho_batch)\n",
    "dataset_teste = dataset.skip(tamanho_treinamento).batch(tamanho_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "otimizador = tf.keras.optimizers.Adam(learning_rate=taxa_aprendizagem)\n",
    "perda = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.layers[-1].units = camadas\n",
    "modelo.layers[-1].activation = tf.keras.activations.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(\n",
    "    optimizer=otimizador, \n",
    "    loss=perda, \n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "modelo.fit(\n",
    "    dataset_treinamento, \n",
    "    epochs=epocas, \n",
    "    validation_data=dataset_teste\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folder(folder_path):\n",
    "    files_list = list()\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            files_list.append(file_path)\n",
    "    \n",
    "    return files_list\n",
    "\n",
    "folder_path = diretorio_trabalho + \"raw_zone/datasets/infomoney/\"\n",
    "files_list = list_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contagem, arquivo in enumerate(files_list):\n",
    "    df = pd.read_excel(arquivo)\n",
    "\n",
    "    titulos = df[\"titulo\"]\n",
    "    titulos_classificados = list()\n",
    "\n",
    "    for titulo in titulos:\n",
    "        frase_tokenizada = tokenizador.encode_plus(\n",
    "            titulo,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            max_length=tamanho_bytes,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "        id_teste = frase_tokenizada['input_ids']\n",
    "        mascara_teste = frase_tokenizada['attention_mask']\n",
    "        predicoes = modelo.predict({'input_ids': id_teste, 'attention_mask': mascara_teste})\n",
    "        logits = predicoes.logits\n",
    "        classe_predita = tf.argmax(logits, axis=1).numpy()[0]\n",
    "        titulos_classificados.append(interpertar_predicao(classe_predita))\n",
    "    \n",
    "    df[\"classificacao\"] = titulos_classificados\n",
    "    df.to_excel(diretorio_trabalho + \"model/datasets_classificados/{}.xlsx\".format(contagem + 2007))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
